{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 2024 Online Summer Internship\n",
    "### Name: Rasikh Ali\n",
    "### Email: rasikhali1234@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1> Libraries </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-12T12:15:29.762084Z",
     "iopub.status.busy": "2024-08-12T12:15:29.761521Z",
     "iopub.status.idle": "2024-08-12T12:16:27.237229Z",
     "shell.execute_reply": "2024-08-12T12:16:27.236219Z",
     "shell.execute_reply.started": "2024-08-12T12:15:29.762052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 24.6.1 requires cubinlinker, which is not installed.\n",
      "cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cudf 24.6.1 requires ptxcompiler, which is not installed.\n",
      "cuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "dask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "keras-cv 0.9.0 requires keras-core, which is not installed.\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\n",
      "cudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\n",
      "distributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\n",
      "jupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "osmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "pointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "rapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\n",
      "spaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "spopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers --quiet\n",
    "!pip install accelerate --quiet\n",
    "!pip install bitsandbytes --quiet\n",
    "!pip install langchain --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T12:16:27.239434Z",
     "iopub.status.busy": "2024-08-12T12:16:27.238666Z",
     "iopub.status.idle": "2024-08-12T12:16:41.665667Z",
     "shell.execute_reply": "2024-08-12T12:16:41.664261Z",
     "shell.execute_reply.started": "2024-08-12T12:16:27.239398Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain-community langchain-core --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T12:16:41.667590Z",
     "iopub.status.busy": "2024-08-12T12:16:41.667253Z",
     "iopub.status.idle": "2024-08-12T12:16:42.037179Z",
     "shell.execute_reply": "2024-08-12T12:16:42.036429Z",
     "shell.execute_reply.started": "2024-08-12T12:16:41.667559Z"
    }
   },
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from textwrap import fill\n",
    "from IPython.display import Markdown, display # for formating Python display folowing markdown language\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # avoid warning messages importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T12:16:42.039827Z",
     "iopub.status.busy": "2024-08-12T12:16:42.039469Z",
     "iopub.status.idle": "2024-08-12T12:16:59.789477Z",
     "shell.execute_reply": "2024-08-12T12:16:59.788713Z",
     "shell.execute_reply.started": "2024-08-12T12:16:42.039803Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 12:16:49.608632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-12 12:16:49.608730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-12 12:16:49.741768: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Mistral and LangChain packages (prompt engineering)\n",
    "import torch\n",
    "from langchain import PromptTemplate, HuggingFacePipeline\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1> Loading Model </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T12:16:59.791155Z",
     "iopub.status.busy": "2024-08-12T12:16:59.790586Z",
     "iopub.status.idle": "2024-08-12T12:18:28.895515Z",
     "shell.execute_reply": "2024-08-12T12:18:28.894556Z",
     "shell.execute_reply.started": "2024-08-12T12:16:59.791127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6540bba5551645849f4909632ac96498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model version of Mistral\n",
    "MODEL_NAME = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n",
    "\n",
    "# Quantization is a technique used to reduce the memory and computation requirements \n",
    "# of deep learning models, typically by using fewer bits, 4 bits\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Initialization of a tokenizer for the Mistral-7b model, \n",
    "# necessary to preprocess text data for input\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialization of the pre-trained language Mistral-7b\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# Configuration of some generation-related settings\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 20 # maximum number of new tokens that can be generated by the model\n",
    "generation_config.temperature = 0.7 # randomness of the generated tex\n",
    "generation_config.top_p = 0.95 # diversity of the generated text\n",
    "generation_config.do_sample = True # sampling during the generation process\n",
    "generation_config.repetition_penalty = 1.15 # the degree to which the model should avoid repeating tokens in the generated text\n",
    "\n",
    "# A pipeline is an object that works as an API for calling the model\n",
    "# The pipeline is made of (1) the tokenizer instance, the model instance, and\n",
    "# some post-procesing settings. Here, it's configured to return full-text outputs\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T12:18:28.897831Z",
     "iopub.status.busy": "2024-08-12T12:18:28.897525Z",
     "iopub.status.idle": "2024-08-12T12:18:29.360022Z",
     "shell.execute_reply": "2024-08-12T12:18:29.359116Z",
     "shell.execute_reply.started": "2024-08-12T12:18:28.897805Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace pipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1> Loading Dataset </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T12:18:29.382146Z",
     "iopub.status.busy": "2024-08-12T12:18:29.381871Z",
     "iopub.status.idle": "2024-08-12T12:18:29.412891Z",
     "shell.execute_reply": "2024-08-12T12:18:29.412039Z",
     "shell.execute_reply.started": "2024-08-12T12:18:29.382124Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "data= pd.read_csv('/kaggle/input/assingment-5-abstract-multi-label-dataset/Test_Sample.csv')\n",
    "data_t = data.copy()\n",
    "#Extract text from dataset\n",
    "test_texts = data_t['ABSTRACT'].tolist()\n",
    "\n",
    "#Extract actual_values for abstract\n",
    "test_labels = data_t.drop(columns=['TITLE', 'ID', 'ABSTRACT']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <h1> Defining Labels </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T12:18:29.361502Z",
     "iopub.status.busy": "2024-08-12T12:18:29.361202Z",
     "iopub.status.idle": "2024-08-12T12:18:29.365998Z",
     "shell.execute_reply": "2024-08-12T12:18:29.364964Z",
     "shell.execute_reply.started": "2024-08-12T12:18:29.361471Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Define the abstract labels\n",
    "abstract_labels = [\"Computer Science\", \"Physics\", \"Mathematics\", \"Statistics\", \"Quantitative Biology\", \"Quantitative Finance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <h2> Define function to clean the text </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T12:18:29.367414Z",
     "iopub.status.busy": "2024-08-12T12:18:29.367131Z",
     "iopub.status.idle": "2024-08-12T12:18:29.380815Z",
     "shell.execute_reply": "2024-08-12T12:18:29.380028Z",
     "shell.execute_reply.started": "2024-08-12T12:18:29.367392Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_abstract(text):\n",
    "    \"\"\"Extracts text outside [INST] and [/INST] tags and checks for abstract labels.\"\"\"\n",
    "    last_inst_index = text.rfind(\"[/INST]\")\n",
    "    if last_inst_index != -1:\n",
    "        my_text = text[last_inst_index + len(\"[/INST]\") :].lower().strip()\n",
    "        # Normalize text for easier matching\n",
    "        response_text = re.sub(r'[\\[\\]\\.\\'\\\"\\,]', '', my_text)  # remove certain characters\n",
    "        response_text = re.sub(r'\\s+', ' ', response_text)  # normalize spaces\n",
    "        detected_abstract = response_text.split(\" \")\n",
    "        return [1 if abstract.lower() in detected_abstract else 0 for abstract in abstract_labels]\n",
    "    else:\n",
    "        return [0] * len(abstract_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1> Testing Phase </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T12:18:29.414258Z",
     "iopub.status.busy": "2024-08-12T12:18:29.413996Z",
     "iopub.status.idle": "2024-08-12T12:19:18.969425Z",
     "shell.execute_reply": "2024-08-12T12:19:18.968484Z",
     "shell.execute_reply.started": "2024-08-12T12:18:29.414237Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** [0, 0, 1, 1, 0, 0]\n",
      "**** [0, 1, 1, 0, 0, 0]\n",
      "**** [0, 1, 1, 0, 0, 0]\n",
      "**** [0, 0, 1, 0, 0, 0]\n",
      "**** [0, 0, 1, 1, 0, 0]\n",
      "**** [0, 1, 0, 0, 0, 0]\n",
      "**** [0, 0, 0, 0, 0, 0]\n",
      "**** [0, 1, 1, 0, 0, 0]\n",
      "**** [0, 1, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** [0, 0, 0, 0, 0, 0]\n",
      "**** [0, 1, 0, 0, 0, 0]\n",
      "**** [0, 0, 0, 0, 0, 0]\n",
      "**** [0, 1, 1, 0, 0, 0]\n",
      "**** [0, 0, 0, 0, 0, 0]\n",
      "**** [0, 1, 1, 0, 0, 0]\n",
      "**** [0, 1, 1, 1, 0, 0]\n",
      "**** [0, 0, 1, 1, 0, 0]\n",
      "**** [0, 0, 1, 1, 0, 0]\n",
      "**** [0, 1, 1, 0, 0, 0]\n",
      "**** [0, 0, 0, 0, 0, 0]\n",
      "**** [0, 0, 0, 0, 0, 0]\n",
      "**** [0, 0, 1, 0, 0, 0]\n",
      "**** [0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for text in test_texts:\n",
    "    #text = \"i want to go out  of this house\"\n",
    "    # In English\n",
    "    query = f\"\"\"[INST]You are an expert who needs to assess and detect abstract labels present in the given text.\n",
    "            Please identify and list up to four prominent abstract labels present in the provided English text.\n",
    "            Only consider the following abstract labels:\n",
    "\n",
    "            Computer Science\n",
    "            Physics\n",
    "            Mathematics\n",
    "            Statistics\n",
    "            Quantitative Biology\n",
    "            Quantitative Finance\n",
    "\n",
    "            If no abstracts are detected or the text is unclear, label as: ['neutral'].\n",
    "\n",
    "            Your answer should be in the form of a Python list containing only the above-mentioned abstract labels.\n",
    "            Respond with just the labels in list format and nothing else.\n",
    "            Text: {text}\n",
    "            Abstract: \n",
    "            .[/INST] \"\"\"\n",
    "\n",
    "    result = llm(query)\n",
    "#     display(Markdown(f\"<b>{query.removeprefix('[INST]').removesuffix('[/INST]')}</b>\"))\n",
    "#     display(Markdown(f\"<p>{result}</p>\"))\n",
    "    r_label = extract_abstract(result)\n",
    "    print(\"****\", r_label)\n",
    "    predictions.append(r_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T12:19:18.971353Z",
     "iopub.status.busy": "2024-08-12T12:19:18.970980Z",
     "iopub.status.idle": "2024-08-12T12:19:18.978672Z",
     "shell.execute_reply": "2024-08-12T12:19:18.977756Z",
     "shell.execute_reply.started": "2024-08-12T12:19:18.971321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 0 0] \n",
      " [0, 0, 1, 1, 0, 0] \n",
      "\n",
      "\n",
      "[0 1 1 0 0 0] \n",
      " [0, 1, 1, 0, 0, 0] \n",
      "\n",
      "\n",
      "[1 0 0 1 0 0] \n",
      " [0, 1, 1, 0, 0, 0] \n",
      "\n",
      "\n",
      "[0 0 1 0 0 0] \n",
      " [0, 0, 1, 0, 0, 0] \n",
      "\n",
      "\n",
      "[1 0 0 0 0 0] \n",
      " [0, 0, 1, 1, 0, 0] \n",
      "\n",
      "\n",
      "[1 0 1 0 0 0] \n",
      " [0, 1, 0, 0, 0, 0] \n",
      "\n",
      "\n",
      "[0 1 0 0 0 0] \n",
      " [0, 0, 0, 0, 0, 0] \n",
      "\n",
      "\n",
      "[0 1 0 0 0 0] \n",
      " [0, 1, 1, 0, 0, 0] \n",
      "\n",
      "\n",
      "[1 0 1 0 0 0] \n",
      " [0, 1, 0, 0, 0, 0] \n",
      "\n",
      "\n",
      "[1 0 0 0 0 0] \n",
      " [0, 0, 0, 0, 0, 0] \n",
      "\n",
      "\n",
      "[0 1 0 0 0 0] \n",
      " [0, 1, 0, 0, 0, 0] \n",
      "\n",
      "\n",
      "[1 0 0 0 0 0] \n",
      " [0, 0, 0, 0, 0, 0] \n",
      "\n",
      "\n",
      "[1 0 0 1 0 0] \n",
      " [0, 1, 1, 0, 0, 0] \n",
      "\n",
      "\n",
      "[1 0 0 0 0 0] \n",
      " [0, 0, 0, 0, 0, 0] \n",
      "\n",
      "\n",
      "[0 0 1 0 0 0] \n",
      " [0, 1, 1, 0, 0, 0] \n",
      "\n",
      "\n",
      "[1 0 0 1 0 0] \n",
      " [0, 1, 1, 1, 0, 0] \n",
      "\n",
      "\n",
      "[0 0 1 0 0 0] \n",
      " [0, 0, 1, 1, 0, 0] \n",
      "\n",
      "\n",
      "[0 0 1 1 0 0] \n",
      " [0, 0, 1, 1, 0, 0] \n",
      "\n",
      "\n",
      "[0 1 0 0 0 0] \n",
      " [0, 1, 1, 0, 0, 0] \n",
      "\n",
      "\n",
      "[1 0 1 1 0 0] \n",
      " [0, 0, 0, 0, 0, 0] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(test_labels[i],\"\\n\",predictions[i], \"\\n\\n\")\n",
    "#print(test_labels, \"\\n\\n\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <h1> Processing Results </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T12:19:18.979988Z",
     "iopub.status.busy": "2024-08-12T12:19:18.979716Z",
     "iopub.status.idle": "2024-08-12T12:19:18.991124Z",
     "shell.execute_reply": "2024-08-12T12:19:18.990214Z",
     "shell.execute_reply.started": "2024-08-12T12:19:18.979965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_labels: 23\n",
      "Length of predictions: 23\n"
     ]
    }
   ],
   "source": [
    "# Check the lengths of test_labels and predictions\n",
    "print(f\"Length of test_labels: {len(test_labels)}\")\n",
    "print(f\"Length of predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T12:19:18.994156Z",
     "iopub.status.busy": "2024-08-12T12:19:18.993836Z",
     "iopub.status.idle": "2024-08-12T12:19:19.004386Z",
     "shell.execute_reply": "2024-08-12T12:19:19.003561Z",
     "shell.execute_reply.started": "2024-08-12T12:19:18.994133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test_labels: 23\n",
      "Length of predictions: 23\n"
     ]
    }
   ],
   "source": [
    "# If lengths are inconsistent, fix them\n",
    "if len(test_labels) != len(predictions):\n",
    "    # Align the arrays by trimming the longer array\n",
    "    min_length = min(len(test_labels), len(predictions))\n",
    "    test_labels = test_labels[:min_length]\n",
    "    predictions = predictions[:min_length]\n",
    "    \n",
    "# Check the lengths of test_labels and predictions\n",
    "print(f\"Length of test_labels: {len(test_labels)}\")\n",
    "print(f\"Length of predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1> Displaying Accuracy </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T12:19:19.006173Z",
     "iopub.status.busy": "2024-08-12T12:19:19.005555Z",
     "iopub.status.idle": "2024-08-12T12:19:19.035099Z",
     "shell.execute_reply": "2024-08-12T12:19:19.034166Z",
     "shell.execute_reply.started": "2024-08-12T12:19:19.006142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.43750000000000006\n",
      "F1 Macro: 0.26795199685225257\n",
      "Precision Micro: 0.4827586206896552\n",
      "Precision Macro: 0.2547008547008547\n",
      "Recall Micro: 0.4\n",
      "Recall Macro: 0.29047619047619044\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "# Compute evaluation metrics\n",
    "f1_micro = f1_score(test_labels, predictions, average='micro')\n",
    "f1_macro = f1_score(test_labels, predictions, average='macro')\n",
    "precision_micro = precision_score(test_labels, predictions, average='micro')\n",
    "precision_macro = precision_score(test_labels, predictions, average='macro')\n",
    "recall_micro = recall_score(test_labels, predictions, average='micro')\n",
    "recall_macro = recall_score(test_labels, predictions, average='macro')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"F1 Micro: {f1_micro}\")\n",
    "print(f\"F1 Macro: {f1_macro}\")\n",
    "print(f\"Precision Micro: {precision_micro}\")\n",
    "print(f\"Precision Macro: {precision_macro}\")\n",
    "print(f\"Recall Micro: {recall_micro}\")\n",
    "print(f\"Recall Macro: {recall_macro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5490690,
     "sourceId": 9097970,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5532617,
     "sourceId": 9158044,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 1902,
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
